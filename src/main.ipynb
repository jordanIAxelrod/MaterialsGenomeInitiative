{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83e2458b",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8abd43e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-19T22:02:54.951575Z",
     "start_time": "2022-11-19T22:02:53.766677Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sentence_level_preprocess import *\n",
    "from word_level_preprocess import *\n",
    "from indexation import *\n",
    "from featurize import *\n",
    "from embeddings import Embeddings\n",
    "\n",
    "from typing import Tuple, List\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99424037",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_records = List[Tuple]\n",
    "\n",
    "def load_data(path: str) -> numpy_records:\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    return df.to_records(index=False)\n",
    "\n",
    "def preprocess(data: numpy_records) -> numpy_records:\n",
    "    # remove line breaks (e.g. \"elec- tron\" -> \"electron\")\n",
    "    data = [(rm_linebreaks(t), l) for t, l in data]\n",
    "\n",
    "    # lower case\n",
    "    data = [(t.lower(), l) for t, l in data]\n",
    "\n",
    "    # unify expressions for temperature (e.g. '° c' -> '<temp>')\n",
    "    data = [(c2temp_2(t), l) for t, l in data]\n",
    "\n",
    "    data = [(nltk.tokenize.word_tokenize(t), l) for t, l in data]\n",
    "\n",
    "    # recognize integer as '<int>' (e.g. '60' -> '<int>')\n",
    "    data = [([put_int_together(w) for w in t], l) for t, l in data]\n",
    "\n",
    "    # recognize decimal as '<dec>' (e.g. '0.5' -> '<dec>')\n",
    "    data = [([put_decimal_together(w) for w in t], l) for t, l in data]\n",
    "\n",
    "    # recognize ratioas '<ratio>' (e.g. '1:1' -> '<ratio>')\n",
    "    data = [([put_ratio_together(w) for w in t], l) for t, l in data]\n",
    "\n",
    "    # split slash (e.g. 'g/mol' -> '['g', '/', 'mol'])\n",
    "    data = [([split_slash(w) for w in t], l) for t, l in data]\n",
    "    data = [(list(itertools.chain.from_iterable(t)), l) for t, l in data] # flatten\n",
    "\n",
    "    # unify expressions for temperature (e.g. '°c' -> '<temp>')\n",
    "    data = [([c2temp(w) for w in t], l) for t, l in data]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def index_words(data: numpy_records):\n",
    "    texts   = [x[0] for x in data]\n",
    "    targets = [[x[1]] for x in data]\n",
    "    texts, vocab_size, _   = word2idx(texts)\n",
    "    targets, _, _          = word2idx(targets)\n",
    "    targets = [l[0] for l in targets]\n",
    "    data_idx = list(zip(texts, targets))\n",
    "    return (data_idx, vocab_size, targets)\n",
    "\n",
    "def BOW_featurize(data_idx, vocab_size):\n",
    "    return [(bow(t, vocab_size), l) for t, l in data_idx]\n",
    "\n",
    "def train_val_split(data: numpy_records) -> Tuple[List[str]]:\n",
    "    X = [t for t, _ in data]\n",
    "    y = [l for _, l in data]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    return (X_train, X_val, y_train, y_val)\n",
    "\n",
    "def embed(embedding, train_data, val_data):\n",
    "    return embedding.transform(\n",
    "        train_data.text.apply(lambda x: ' '.join([str(y) for y in x]))\n",
    "    ), embedding.transform(\n",
    "        val_data.text.apply(lambda x: ' '.join([str(y) for y in x]))\n",
    "    )\n",
    "\n",
    "# def generate_embeddings(path: str, embed):\n",
    "#     data_idx, vocab_size, targets = index_words(preprocess(load_data(path)))\n",
    "#     X_total = pd.DataFrame(data_idx, columns=['text', 'classification'])\n",
    "#     y_total = X_total.pop('classification')\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X_total, y_total, test_size=0.2)\n",
    "#     embeddings = Embeddings(X_train, X_val)\n",
    "#     X_train, X_val = embed(embed, X_train, X_val)\n",
    "#     return (X_train, X_val, y_train, y_val)\n",
    "\n",
    "def generate_split(path):\n",
    "    data_idx, vocab_size, targets = index_words(preprocess(load_data(path)))\n",
    "    X_total = pd.DataFrame(data_idx, columns=['text', 'classification'])\n",
    "    y_total = X_total.pop('classification')\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_total, y_total, test_size=0.2)\n",
    "    return (X_train, X_val, y_train, y_val)\n",
    "\n",
    "def find_avg_performance(test_model: BaseEstimator, embedding_type, num_trial = 20) -> None:\n",
    "    f1s = []\n",
    "\n",
    "    for i in range(num_trial):\n",
    "        X_train_avg, X_val_avg, y_train_avg, y_val_avg = generate_split('../data/train.tsv')\n",
    "        X_train_avg, X_val_avg = embed(embedding_type, X_train_avg, X_val_avg)\n",
    "        test_model.fit(X_train_avg, y_train_avg)\n",
    "        y_pred = test_model.predict(X_val_avg)\n",
    "        f1 = f1_score(y_val_avg, y_pred, average='micro')\n",
    "        f1s.append(f1)\n",
    "\n",
    "    print(\"mean:\", np.mean(f1s))\n",
    "    print(\"std :\", np.std(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda21f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = generate_split('../data/train.tsv')\n",
    "embeddings = Embeddings(X_train, X_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17ad82a4",
   "metadata": {},
   "source": [
    "### BOW Data\n",
    "\n",
    "Uncomment **one** of these three cells to experiment with that type of word-representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776b6c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# embed_type = embeddings.BOW()\n",
    "# X_train, X_val = embed(embed_type, X_train, X_val)\n",
    "# X_train = StandardScaler().fit_transform(X_train.todense())\n",
    "# X_val = StandardScaler().fit_transform(X_val.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9571148e",
   "metadata": {},
   "source": [
    "### TF-IDF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbb5473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_type = embeddings.TFIDF()\n",
    "# X_train, X_val = embed(embeddings.TFIDF(), X_train, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d09e700",
   "metadata": {},
   "source": [
    "### word2vec Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_type = embeddings.word2vec()\n",
    "# X_train, X_val = embed(embed_type, X_train, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e881f4fe",
   "metadata": {},
   "source": [
    "# Learn Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf06951",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ec758",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-19T22:02:56.451369Z",
     "start_time": "2022-11-19T22:02:55.833269Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f746a",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b17b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-19T22:02:57.921546Z",
     "start_time": "2022-11-19T22:02:56.502867Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e48951",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-19T22:02:57.967075Z",
     "start_time": "2022-11-19T22:02:57.924990Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "cm = confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a24ebc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-19T22:02:58.321837Z",
     "start_time": "2022-11-19T22:02:57.970701Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a1e03e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-19T22:02:58.530209Z",
     "start_time": "2022-11-19T22:02:58.326281Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(cm / np.tile(np.sum(cm, axis=1), (4, 1)).T, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b14c934",
   "metadata": {},
   "source": [
    "### Average performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44992816",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-19T22:03:10.534357Z",
     "start_time": "2022-11-19T22:02:58.552332Z"
    }
   },
   "outputs": [],
   "source": [
    "find_avg_performance(LogisticRegression(max_iter=5000), embed_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68853fcd",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c8c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90248930",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lr = {\n",
    "    'C': uniform(0.001, 1000)\n",
    "}\n",
    "\n",
    "scores = []\n",
    "\n",
    "lr1_grid = RandomizedSearchCV(\n",
    "    LogisticRegression(solver='saga', max_iter=2000, penalty='l1'), \n",
    "    param_distributions=param_grid_lr, \n",
    "    n_jobs=-1, \n",
    "    scoring='f1_micro'\n",
    ")\n",
    "lr1_grid.fit(X_train, y_train)\n",
    "scores.append(('L1', lr1_grid.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b3daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2_grid = RandomizedSearchCV(\n",
    "    LogisticRegression(solver='saga', max_iter=1000, penalty='l2'), \n",
    "    param_distributions=param_grid_lr, \n",
    "    n_jobs=-1, \n",
    "    scoring='f1_micro'\n",
    ")\n",
    "lr2_grid.fit(X_train, y_train)\n",
    "scores.append(('L2', lr2_grid.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a30a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lr['l1_ratio'] = uniform(0,1)\n",
    "lre_grid = RandomizedSearchCV(\n",
    "    LogisticRegression(solver='saga', max_iter=1000, penalty='elasticnet'), \n",
    "    param_distributions=param_grid_lr, \n",
    "    n_jobs=-1, \n",
    "    scoring='f1_micro'\n",
    ")\n",
    "lre_grid.fit(X_train, y_train)\n",
    "scores.append(('Elastic', lre_grid.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8942457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pen, score in scores:\n",
    "    print(f'{pen}: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0507b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_avg_performance(lr1_grid.best_estimator_, embed_type)\n",
    "find_avg_performance(lr2_grid.best_estimator_, embed_type)\n",
    "find_avg_performance(lre_grid.best_estimator_, embed_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f426a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_grid.predict(X_val)\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6424d4f",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f0f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_model.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a97d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_svm = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], \n",
    "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001, 'scale']\n",
    "}\n",
    "\n",
    "svm_grid = GridSearchCV(SVC(), param_grid_svm, refit=True, n_jobs=-1, scoring='f1_micro')\n",
    "svm_grid.fit(X_train, y_train)\n",
    "svm_grid.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082e528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_grid.predict(X_val)\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e830847b",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f635ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network import RNN, NNTrainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df08b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.concat((X_train, y_train))\n",
    "val_set = pd.concat((X_val, y_val))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=10, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff7811",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nn_embed = RNN(4, 50, num_words, 100, False, word_types)\n",
    "nn_w2v = RNN(4, 50, num_words, 100, False, word_types)\n",
    "nn_w2v.set_embedding_weights()\n",
    "nn_trainer_e = NNTrainer(\n",
    "    nn_embed,\n",
    "    5,\n",
    "    torch.optim.Adam,\n",
    "    torch.nn.CrossEntropyLoss(),\n",
    "    train_loader,\n",
    "    val_loader\n",
    ")\n",
    "nn_trainer_w = NNTrainer(\n",
    "    nn_embed,\n",
    "    5,\n",
    "    torch.optim.Adam,\n",
    "    torch.nn.CrossEntropyLoss(),\n",
    "    train_loader,\n",
    "    val_loader\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('pollydarton')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov  4 2022, 16:13:54) \n[GCC 11.2.0]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "a9b78b718931f80fc5e3ab72ed521239234f1386c1c68376b39676895c1cd556"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
